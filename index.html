
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0//EN">
<html>
<head>
<title>Shizhe Chen</title>
<style type="text/css">
    body
    {
        width:1200px;
        text-align: center;
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
        font-weight: 300;
        font-size:14px;
        background-color: #FFF;
    }
    
    table
    {
        padding: 5px;
    }
    
    table.pub_table,td.pub_td1,td.pub_td2
    {
        border-collapse: collapse;
        border-bottom: 0px solid #9B9B9B;
        padding-bottom: 10px;
        padding-top: 10px;
        padding-left: 10px;
        width: 800px;
    }
    td.pub_td1
    {
        width:100px;
    }
    td.pub_td2
    {
    }
    tr {
        background-color: #FFF;
    }
    
    div#container
    {
        margin-left: auto;
        margin-right: auto;
        width: 900px;
        text-align: left;
        position: relative;
        background-color: #F3F3F3;
    }
    div#DocInfo
    {
        color: #9B9B9B;
        height: 128px;
    }
    h4,h3,h2,h1
    {
        color: #3B3B3B;
    }
    h2
    {
        font-size:130%;
    }
    p
    {
        color: #5B5B5B;
        margin-bottom: 20px;
    }
    p.caption
    {
        color: #9B9B9B;
        text-align: left;
        width: 600px;
        font:11px helvetica,sans-serif;
    }
    p.caption2
    {
        color: #9B9B9B;
        text-align: left;
        width: 800px;
        font:11px helvetica,sans-serif;
    }
    #header_img
    {
        position: absolute;
        top: 0px; right: 0px;
    }
    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    .section_div {
        background-color: #FFF;
        padding: 10px 10px 10px 10px;
        margin: 10px 10px 10px 10px;
        border: 1px solid #AAA;
    }
    body {
        background-color: #F3F3F3;
    }
    #personal_info {
        background-color: #FFF;
    }
    

</style>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-24665197-4', 'auto');
  ga('send', 'pageview');

</script>

</head>


<body>
    <div id="container">
        
    <div class='section_div'>
    <table id="personal_info">
    <tr>
    <td><img src="images/photo_of_me.jpg" width=128px style="border: 1px solid black; float:left; margin-right:15px"/></td>
    <td>
    <div id="DocInfo">
        <h1>Shizhe Chen</h1>
        cszhe1 at ruc dot edu dot cn<br>
        <a href="https://scholar.google.com/citations?user=wZhRRy0AAAAJ&hl=en">Google Scholar</a> / <a href="https://github.com/cshizhe">GitHub</a> / <a href="https://www.linkedin.com/in/shizhe-chen-71542291/">Linkedin</a>
    </div><br>
    </td>
    </tr>
    </table>
    
    
    <h2>About me</h2>
    <p>I am a PhD student at Renmin University of China, advised by <a href="http://jin-qin.com/">Professor Qin Jin</a>. My CV can be found <a href="pdfs/my_cv.pdf">here</a>.
        <br><br>
        My research objective is to make the human-machine interactions more natural such as understanding human emotions or communicating with natural language sentences.
        <br><br>
        My research area is in the interdisciplinary field of multimedia content analysis, computer vision, affective computing and deep learning.
    </p>
    </div>
    
    

    
    
    <div class='section_div'>
    <h2>Selected Papers</h2>
    
    <table class="pub_table">
    
    <tr>
        <td class="pub_td1"><img src='images/MM_TGM_thumb.png' width='80px' style='margin-left: 10px; margin-top:3px; border: 0px solid black'/></td>
        <td class="pub_td2">Shizhe Chen, Jia Chen, Qin Jin, Alexander Hauptmann<br><b>Captioning with Guidance of Multimodal Latent Topics</b><br><i>ACM Multimedia</i>, 2017.<br>[<a href="https://arxiv.org/abs/1708.09667">Paper</a>]
    </td></tr>
    
    <tr>
        <td class="pub_td1"><img src='images/MSRVTT17_thumb.png' width='80px' style='margin-left: 10px; margin-top:3px; border: 0px solid black'/></td>
        <td class="pub_td2">Qin Jin, Shizhe Chen, Jia Chen, Alexander Hauptmann<br><b>Knowing Yourself: Improving Video Caption via In-depth Recap</b><br><i>ACM Multimedia</i>, 2017.<br>
    </td></tr>
    
    
    <tr>
        <td class="pub_td1"><img src='images/TGM_thumb.png' width='80px' style='margin-left: 10px; margin-top:3px; border: 0px solid black'/></td>
        <td class="pub_td2">Shizhe Chen, Jia Chen, Qin Jin<br><b>Generating Video Descriptions with Topic Guidance</b><br><i>ICMR</i>, 2017.<br>[<a href="https://arxiv.org/abs/1708.09666">Paper</a>]
    </td></tr>
    
    <tr>
        <td class="pub_td1"><img src='images/CA_fusion_thumb.png' width='80px' style='margin-left: 10px; margin-top:3px; border: 0px solid black'/></td>
        <td class="pub_td2">Shizhe Chen, Qin Jin<br><b>Multi-modal Conditional Attention Fusion for Dimensional Emotion Prediction</b><br><i>ACM Multimedia</i>, 2016.<br>[<a href="https://arxiv.org/abs/1709.02251">Paper</a>]
    </td></tr>
    
    <tr>
        <td class="pub_td1"><img src='images/Emotiw16_thumb.png' width='80px' style='margin-left: 10px; margin-top:3px; border: 0px solid black'/></td>
        <td class="pub_td2"> Shizhe Chen, Xinrui Li, Qin Jin, Shilei Zhang, Yong Qin<br><b>Video Emotion Recognition in the Wild based on Fusion of Multimodal Features</b><br><i>ICMI</i>, 2016.<br>
    </td></tr>

    
    </table>
    
    </div>

    <div class='section_div'>
    <h2>Selected Awards</h2>
    <ul>
        <li>Ranked 1st in NIST Trecvid 2017 Video to Text Task.</li>
        <li>Ranked 1st in ACM Multimedia 2016 and 2017 Video to Language Grand Challenge.</li>
        <li>Ranked 2nd in AVEC 2016 Continuous Emotion Recognition Sub-challenge.</li>
        <li>Ranked 2nd in CCPR 2016 Multimodal Emotion Recognition Challenge.</li>
        <li>Ranked 1st in MediaEval 2016 Emotion Impact of Movies Subtask.</li>
        <li>National Scholarship for Ph.D. Students in 2016.</li>
        <li>ACM Multimedia 2016 Student Travel Grant.</li>
        <li>Second Prize in the Chinese Big Data Contest P2P Fraud Detection Sub-contest 2015.</li>
        <li>Second Prize in IBM Bluemix Cognitive Computation Development Contest 2015.</li>
        <li>First Prize in National College Student Information Security Contest 2014.</li>
        <li>Second Prize in the Chinese Big Data Contest Baidu iErmu Sub-contest 2014.</li>
        <li>Meritorious Winner in American Mathematical Contest in Modeling 2014.</li>
        <li>National Second Prize in China Undergraduate Mathematical Contest in Modeling 2013.</li>
        <li>National Scholarship for Undergraduate Students in 2013.</li>
    </ul>
    </div>

    <footer id="myFooter">
  <div class="w3-row" style="max-width:1500">
    <div class="w3-container w3-third">
      <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=vDq9g-ExkN9FVxQtsZHbYwov_XtBsFmdoxL7C1FiObc&cl=ffffff&w=a"></script>
    </div>
    <div class="w3-container w3-padding-32 w3-twothird">
      <div>Last Updated on Sep, 2017</div>
      <div>Copyright Â© 2017 Shizhe Chen</div>
    </div>
  </div>
</footer>

    
    
    
</body>

</html>